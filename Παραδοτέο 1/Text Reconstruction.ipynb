{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Κείμενο 1:\n",
    "\n",
    "“Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in our lives. Hope you too, to enjoy it as my deepest wishes.Thank your message to show our words to the doctor, as his next contract checking, to all of us.I got this message to see the approved message. In fact, I have received the message from the professor, to show me, this, a couple of days ago. I am very appreciated the full support of the professor, for our Springer proceedings publication”\n",
    "\n",
    "Κείμενο 2:\n",
    "\n",
    "“During our final discuss, I told him about the new submission — the one we were waiting since last autumn, but the updates was confusing as it not included the full feedback from reviewer or maybe editor?Anyway, I believe the team, although bit delay and less communication at recent days, they really tried best for paper and cooperation. We should be grateful, I mean all of us, for the acceptance and efforts until the Springer link came finally last week, I think.Also, kindly remind me please, if the doctor still plan for the acknowledgments section edit before he sending again. Because I didn’t see that part final yet, or maybe I missed, I apologize if so.Overall, let us make sure all are safe and celebrate the outcome with strong coffee and future targets”\n",
    "\n",
    "Παραδοτέο 1: Ανακατασκευή Κειμένου\n",
    "\n",
    "Απο τα παραπάνω κείμενα σας ζητείται να υλοποιήσετε τα εξής:"
   ],
   "id": "3dc08be35eda8bd3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A. Aνακατασκευή 2 προτάσεων της επιλογής σας με αυτόματο που θα διαμορφώσετε εσείς.",
   "id": "b05882311427448f"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-19T19:44:30.039378Z",
     "start_time": "2025-09-19T19:44:30.012822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "# ---------------------------\n",
    "# Tokenization / utils\n",
    "# ---------------------------\n",
    "def tokenize(sent):\n",
    "    s = sent.replace(',', ' , ')\n",
    "    toks = [t.strip().lower() for t in s.split() if t.strip() != ',']\n",
    "    return toks\n",
    "\n",
    "def detokenize(tokens):\n",
    "    if not tokens:\n",
    "        return \"\"\n",
    "    toks = [t.upper() if t == 'i' else t for t in tokens]\n",
    "    s = \" \".join(toks)\n",
    "    if ' with ' in s:\n",
    "        s = s.replace(' with ', ', with ', 1)\n",
    "    if not s.endswith('.'):\n",
    "        s = s + '.'\n",
    "    s = s.replace('..', '.')\n",
    "    return s\n",
    "\n",
    "def levenshtein(a, b):\n",
    "    la, lb = len(a), len(b)\n",
    "    dp = [[0]*(lb+1) for _ in range(la+1)]\n",
    "    for i in range(la+1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(lb+1):\n",
    "        dp[0][j] = j\n",
    "    for i in range(1, la+1):\n",
    "        for j in range(1, lb+1):\n",
    "            cost = 0 if a[i-1] == b[j-1] else 1\n",
    "            dp[i][j] = min(dp[i-1][j] + 1,\n",
    "                           dp[i][j-1] + 1,\n",
    "                           dp[i-1][j-1] + cost)\n",
    "    return dp[la][lb]\n",
    "\n",
    "VOCAB = [\n",
    "    'i', 'you', 'we', 'it',\n",
    "    'hope', 'enjoy', 'too', 'with', 'my', 'deepest', 'wishes',\n",
    "    'got', 'your', 'message', 'to', 'review', 'the', 'approved', 'content',\n",
    "    'see', 'this', 'a', 'is', 'was', 'that', 'part', 'missed', 'apologize'\n",
    "]\n",
    "\n",
    "BIGRAM = {\n",
    "    ('<s>', 'i'): 0.6,\n",
    "    ('<s>', 'hope'): 0.05,\n",
    "    ('<s>', 'got'): 0.05,\n",
    "    ('i', 'hope'): 0.6,\n",
    "    ('hope', 'you'): 0.5,\n",
    "    ('you', 'enjoy'): 0.55,\n",
    "    ('enjoy', 'it'): 0.6,\n",
    "    ('it', 'too'): 0.45,\n",
    "    ('too', 'with'): 0.35,\n",
    "    ('with', 'my'): 0.7,\n",
    "    ('my', 'deepest'): 0.75,\n",
    "    ('deepest', 'wishes'): 0.75,\n",
    "\n",
    "    # chains for sentence 2\n",
    "    ('i', 'got'): 0.55,\n",
    "    ('got', 'your'): 0.5,\n",
    "    ('your', 'message'): 0.55,\n",
    "    ('message', 'to'): 0.3,\n",
    "    ('to', 'review'): 0.5,\n",
    "    ('review', 'the'): 0.45,\n",
    "    ('the', 'approved'): 0.5,\n",
    "    ('approved', 'content'): 0.7\n",
    "}\n",
    "\n",
    "BIGRAM_DEFAULT = 1e-3\n",
    "\n",
    "PCFG_PROBS = {\n",
    "    'S->NP_VP': 0.8,\n",
    "    'S->VP_NP': 0.2,\n",
    "    'NP->Pron': 0.4,\n",
    "    'NP->Det_N1': 0.6,\n",
    "    'N1->Adj_N': 0.7,\n",
    "    'N1->N': 0.3,\n",
    "    'VP->V_NP_ADV_PP': 0.5,\n",
    "    'VP->V_NP_PP': 0.2,\n",
    "    'VP->V_NP_ADV': 0.15,\n",
    "    'VP->V_NP': 0.15,\n",
    "    'Pron=i': 0.5,\n",
    "    'Pron=you': 0.35,\n",
    "    'V=hope': 0.6,\n",
    "    'V=enjoy': 0.5,\n",
    "    'V=got': 0.6,\n",
    "    'Word=review': 0.6,\n",
    "    'Word=content': 0.6,\n",
    "}\n",
    "\n",
    "SMOOTH_K = 5e-4\n",
    "TEMPERATURE = 1.0\n",
    "\n",
    "def generate_candidates(tokens):\n",
    "    edit_ops = [\n",
    "        'insert_I',\n",
    "        'delete_first_too',\n",
    "        'delete_to_before_enjoy',\n",
    "        'move_too_after_it',\n",
    "        'sub_as_to_with',\n",
    "        'sub_this_to_your',\n",
    "        'sub_see_to_review',\n",
    "        'sub_last_message_to_content'\n",
    "    ]\n",
    "\n",
    "    def apply_flags(toks, flags):\n",
    "        toks = toks.copy()\n",
    "        # substitutions\n",
    "        if flags.get('sub_as_to_with'):\n",
    "            toks = ['with' if x == 'as' else x for x in toks]\n",
    "        if flags.get('sub_this_to_your'):\n",
    "            toks = ['your' if x == 'this' else x for x in toks]\n",
    "        if flags.get('sub_see_to_review'):\n",
    "            toks = ['review' if x == 'see' else x for x in toks]\n",
    "        if flags.get('sub_last_message_to_content'):\n",
    "            # replace last 'message' with 'content'\n",
    "            for i in range(len(toks)-1, -1, -1):\n",
    "                if toks[i] == 'message':\n",
    "                    toks[i] = 'content'\n",
    "                    break\n",
    "\n",
    "        # deletions / moves\n",
    "        if flags.get('delete_first_too'):\n",
    "            if 'too' in toks:\n",
    "                toks.remove('too')\n",
    "        if flags.get('delete_to_before_enjoy'):\n",
    "            for i in range(len(toks)-1):\n",
    "                if toks[i] == 'to' and toks[i+1] == 'enjoy':\n",
    "                    toks.pop(i)\n",
    "                    break\n",
    "        if flags.get('move_too_after_it'):\n",
    "            if 'too' in toks:\n",
    "                toks.remove('too')\n",
    "            if 'it' in toks:\n",
    "                idx = toks.index('it')\n",
    "                toks.insert(idx+1, 'too')\n",
    "            else:\n",
    "                toks.append('too')\n",
    "\n",
    "        if flags.get('insert_I'):\n",
    "            toks = ['i'] + toks\n",
    "\n",
    "        return toks\n",
    "\n",
    "    candidates = {}\n",
    "    for mask in range(1 << len(edit_ops)):\n",
    "        flags = {op: bool(mask & (1 << i)) for i, op in enumerate(edit_ops)}\n",
    "        cand = tuple(apply_flags(tokens, flags))\n",
    "        candidates[cand] = flags  # keep one flag set per unique tokens\n",
    "    candidates[tuple(tokens)] = {op: False for op in edit_ops}  # include original\n",
    "    return [(list(k), v) for k, v in candidates.items()]\n",
    "\n",
    "\n",
    "# small unigram-like backoff distribution from BIGRAM totals\n",
    "_UNIGRAM_SUM = {}\n",
    "_TOTAL = 0.0\n",
    "for (a, b), score in BIGRAM.items():\n",
    "    _UNIGRAM_SUM[a] = _UNIGRAM_SUM.get(a, 0.0) + score\n",
    "    _TOTAL += score\n",
    "UNIGRAM_BACKOFF = {}\n",
    "if _TOTAL > 0:\n",
    "    for a, c in _UNIGRAM_SUM.items():\n",
    "        UNIGRAM_BACKOFF[a] = c / _TOTAL\n",
    "\n",
    "def bigram_cond_prob(prev, w, vocab_list=VOCAB):\n",
    "    raw = BIGRAM.get((prev, w), 0.0)\n",
    "    # approximate denominator by summing explicit bigram masses for this prev over our vocab\n",
    "    denom = sum(BIGRAM.get((prev, x), 0.0) for x in vocab_list)\n",
    "    V = max(len(vocab_list), 50)\n",
    "    smoothed = (raw + SMOOTH_K) / (denom + SMOOTH_K * V)\n",
    "    # interpolate\n",
    "    alpha = 0.15\n",
    "    back = UNIGRAM_BACKOFF.get(prev, 1.0 / V)\n",
    "    return (1.0 - alpha) * smoothed + alpha * back\n",
    "\n",
    "def bigram_score(tokens, temp=TEMPERATURE):\n",
    "    if not tokens:\n",
    "        return 1.0\n",
    "    logp = 0.0\n",
    "    prev = '<s>'\n",
    "    for w in tokens:\n",
    "        p = bigram_cond_prob(prev, w)\n",
    "        logp += math.log(max(p, 1e-300))\n",
    "        prev = w\n",
    "    logp = logp / temp\n",
    "    return math.exp(logp)\n",
    "\n",
    "def structure_score(tokens):\n",
    "    if not tokens:\n",
    "        return 1e-12\n",
    "    prob = 1.0\n",
    "    first = tokens[0]\n",
    "    if first in ('i', 'you', 'we', 'it'):\n",
    "        prob *= PCFG_PROBS.get('S->NP_VP', 0.5) * PCFG_PROBS.get('NP->Pron', 0.35)\n",
    "        prob *= PCFG_PROBS.get(f'Pron={first}', 0.1) if f'Pron={first}' in PCFG_PROBS else 0.5\n",
    "    else:\n",
    "        prob *= PCFG_PROBS.get('S->VP_NP', 0.2)\n",
    "    # lexical boosts\n",
    "    for t in tokens:\n",
    "        if PCFG_PROBS.get(f'V={t}') is not None:\n",
    "            prob *= PCFG_PROBS[f'V={t}']\n",
    "        if PCFG_PROBS.get(f'Word={t}') is not None:\n",
    "            prob *= PCFG_PROBS[f'Word={t}']\n",
    "    # reward target chunks\n",
    "    chunk1 = ['with', 'my', 'deepest', 'wishes']\n",
    "    if any(tokens[i:i+4] == chunk1 for i in range(len(tokens)-3)):\n",
    "        prob *= PCFG_PROBS.get('VP->V_NP_ADV_PP', 0.5)\n",
    "    chunk2 = ['to', 'review', 'the', 'approved', 'content']\n",
    "    if any(tokens[i:i+5] == chunk2 for i in range(len(tokens)-4)):\n",
    "        prob *= 1.3\n",
    "    return max(prob, 1e-12)\n",
    "\n",
    "\n",
    "def softmax_from_logits(logits, temp=1.0):\n",
    "    mx = max(logits)\n",
    "    exps = [math.exp((x - mx) / temp) for x in logits]\n",
    "    s = sum(exps)\n",
    "    return [e / s for e in exps]\n",
    "\n",
    "def rank_and_sample_candidates(scored_candidates, sample_n=0, sample_temp=1.0):\n",
    "    logits = [c['score'] for c in scored_candidates]\n",
    "    posterior = softmax_from_logits(logits, temp=sample_temp)\n",
    "    for c, p in zip(scored_candidates, posterior):\n",
    "        c['posterior'] = p\n",
    "    sorted_cands = sorted(scored_candidates, key=lambda x: x['score'], reverse=True)\n",
    "    sampled = []\n",
    "    if sample_n > 0:\n",
    "        for _ in range(sample_n):\n",
    "            r = random.random()\n",
    "            acc = 0.0\n",
    "            for i, p in enumerate(posterior):\n",
    "                acc += p\n",
    "                if r <= acc:\n",
    "                    sampled.append(scored_candidates[i])\n",
    "                    break\n",
    "    return sorted_cands, sampled\n",
    "\n",
    "def rewrite_sentence(src_sentence, alpha=1.0, beta=1.0, gamma=1.0, temp=TEMPERATURE,\n",
    "                     sample_n=0, sample_temp=1.0):\n",
    "    src_tokens = tokenize(src_sentence)\n",
    "    cands = generate_candidates(src_tokens)\n",
    "    scored = []\n",
    "    for toks, flags in cands:\n",
    "        struct_p = structure_score(toks)\n",
    "        bigram_p = bigram_score(toks, temp=temp)\n",
    "        ed = levenshtein(src_tokens, toks)\n",
    "        # log components\n",
    "        struct_log = math.log(max(struct_p, 1e-300))\n",
    "        bigram_log = math.log(max(bigram_p, 1e-300))\n",
    "        # combined log-score (higher is better)\n",
    "        combined_log = alpha * struct_log + beta * bigram_log - gamma * ed\n",
    "        scored.append({\n",
    "            'tokens': toks,\n",
    "            'flags': flags,\n",
    "            'structure_prob': struct_p,\n",
    "            'structure_log': struct_log,\n",
    "            'bigram_prob': bigram_p,\n",
    "            'bigram_log': bigram_log,\n",
    "            'edit_cost': ed,\n",
    "            'score': combined_log\n",
    "        })\n",
    "\n",
    "    sorted_cands, sampled = rank_and_sample_candidates(scored, sample_n=sample_n, sample_temp=sample_temp)\n",
    "    best = sorted_cands[0] if sorted_cands else None\n",
    "    return best, sorted_cands, sampled\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    inputs = [\n",
    "        \"Hope you too, to enjoy it as my deepest wishes\",\n",
    "        \"I got this message to see the approved message\"\n",
    "    ]\n",
    "\n",
    "    alpha = 1.0    # structure weight\n",
    "    beta = 1.0     # bigram weight\n",
    "    gamma = 0.8    # edit penalty weight\n",
    "    temp = 1.0     # bigram temperature\n",
    "    sample_n = 5   # number of stochastic samples to draw (0 disables)\n",
    "    sample_temp = 0.8  # sampling softmax temperature\n",
    "\n",
    "    outputs = []\n",
    "    for src in inputs:\n",
    "        best, ranked, sampled = rewrite_sentence(src, alpha=alpha, beta=beta, gamma=gamma, temp=temp,\n",
    "                                                 sample_n=sample_n, sample_temp=sample_temp)\n",
    "        print(\"INPUT:\", src)\n",
    "        if best:\n",
    "            print(\"BEST (deterministic argmax):\", detokenize(best['tokens']))\n",
    "            print(\"  breakdown: struct_log={:.4f}, bigram_log={:.4f}, edit_cost={}, combined_score={:.4f}, posterior={:.4f}\".format(\n",
    "                best['structure_log'], best['bigram_log'], best['edit_cost'], best['score'], best.get('posterior', 0.0)\n",
    "            ))\n",
    "            outputs.append(detokenize(best['tokens']))\n",
    "        else:\n",
    "            print(\"No candidate produced.\")\n",
    "        print(\"\\nTop 6 candidates (detokenized) with scores and posterior:\")\n",
    "        for c in ranked[:6]:\n",
    "            print(\"  {: .4f}  ed={}  p_struct={:.4g} p_bigram={:.4g} posterior={:.4g} -> {}\".format(\n",
    "                c['score'], c['edit_cost'], c['structure_prob'], c['bigram_prob'], c.get('posterior', 0.0),\n",
    "                detokenize(c['tokens'])\n",
    "            ))\n",
    "        if sampled:\n",
    "            print(\"\\nSampled outputs (stochastic):\")\n",
    "            for s in sampled:\n",
    "                print(\"  posterior={:.4g} -> {}\".format(s.get('posterior', 0.0), detokenize(s['tokens'])))\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    with open(\"outputs_of_A.txt\", \"w\") as f:\n",
    "        for i, output in enumerate(outputs):\n",
    "            f.write(f\"Output {i+1}:\\n{output}\\n\\n\")\n",
    "\n",
    "    print(\"All outputs have been written to outputs_of_A.txt\")"
   ],
   "id": "4cc9d7e0f1ac73bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: Hope you too, to enjoy it as my deepest wishes\n",
      "BEST (deterministic argmax): I hope you enjoy it too, with my deepest wishes.\n",
      "  breakdown: struct_log=-3.7297, bigram_log=-2.6965, edit_cost=5, combined_score=-10.4262, posterior=0.6467\n",
      "\n",
      "Top 6 candidates (detokenized) with scores and posterior:\n",
      "  -10.4262  ed=5  p_struct=0.024 p_bigram=0.06744 posterior=0.6467 -> I hope you enjoy it too, with my deepest wishes.\n",
      "  -10.9356  ed=4  p_struct=0.03 p_bigram=0.01457 posterior=0.3421 -> hope you enjoy it too, with my deepest wishes.\n",
      "  -14.0693  ed=4  p_struct=0.024 p_bigram=0.000793 posterior=0.006807 -> I hope you enjoy it, with my deepest wishes.\n",
      "  -14.5787  ed=3  p_struct=0.03 p_bigram=0.0001713 posterior=0.003601 -> hope you enjoy it, with my deepest wishes.\n",
      "  -16.3040  ed=3  p_struct=0.048 p_bigram=1.907e-05 posterior=0.0004167 -> I hope you enjoy it as my deepest wishes.\n",
      "  -16.8134  ed=2  p_struct=0.06 p_bigram=4.119e-06 posterior=0.0002205 -> hope you enjoy it as my deepest wishes.\n",
      "\n",
      "Sampled outputs (stochastic):\n",
      "  posterior=0.3421 -> hope you enjoy it too, with my deepest wishes.\n",
      "  posterior=0.6467 -> I hope you enjoy it too, with my deepest wishes.\n",
      "  posterior=0.6467 -> I hope you enjoy it too, with my deepest wishes.\n",
      "  posterior=0.6467 -> I hope you enjoy it too, with my deepest wishes.\n",
      "  posterior=0.3421 -> hope you enjoy it too, with my deepest wishes.\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: I got this message to see the approved message\n",
      "BEST (deterministic argmax): I got your message to review the approved content.\n",
      "  breakdown: struct_log=-3.1027, bigram_log=-2.6402, edit_cost=3, combined_score=-8.1429, posterior=0.9783\n",
      "\n",
      "Top 6 candidates (detokenized) with scores and posterior:\n",
      "  -8.1429  ed=3  p_struct=0.04493 p_bigram=0.07135 posterior=0.9783 -> I got your message to review the approved content.\n",
      "  -11.4123  ed=2  p_struct=0.0576 p_bigram=0.0009509 posterior=0.01643 -> I got your message to review the approved message.\n",
      "  -12.8549  ed=4  p_struct=0.04493 p_bigram=0.001427 posterior=0.002707 -> I got your message to review the approved content too.\n",
      "  -12.9821  ed=4  p_struct=0.04493 p_bigram=0.001257 posterior=0.002309 -> I I got your message to review the approved content.\n",
      "  -15.3886  ed=2  p_struct=0.0576 p_bigram=1.783e-05 posterior=0.000114 -> I got your message to see the approved content.\n",
      "  -15.6482  ed=2  p_struct=0.04493 p_bigram=1.764e-05 posterior=8.243e-05 -> I got this message to review the approved content.\n",
      "\n",
      "Sampled outputs (stochastic):\n",
      "  posterior=0.9783 -> I got your message to review the approved content.\n",
      "  posterior=0.9783 -> I got your message to review the approved content.\n",
      "  posterior=0.9783 -> I got your message to review the approved content.\n",
      "  posterior=0.9783 -> I got your message to review the approved content.\n",
      "  posterior=0.9783 -> I got your message to review the approved content.\n",
      "--------------------------------------------------------------------------------\n",
      "All outputs have been written to outputs_of_A.txt\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "B. Ανακατασκευή του συνόλου των 2 κειμένων με χρήση 3 διαφορετικών αυτόματων βιβλιοθηκών python pipelines.",
   "id": "d59844b915933ca5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T21:32:20.362012Z",
     "start_time": "2025-09-22T21:30:04.894840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def paraphrase_text(include_paraphrase, include_dot, cut_at_question_mark, pipeline, input_text, **pipeline_args):\n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "    print(\"Original Text:\\n\" + input_text + \"\\n\")\n",
    "    if not cut_at_question_mark: sentences = input_text.split('. ')\n",
    "    else: sentences = input_text.replace('? ', '. ').split('. ')\n",
    "    corrected_sentences = []\n",
    "\n",
    "    print(\"Processing sentences: \\n\")\n",
    "    bar_len = 30\n",
    "    filled_len = 0\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "    print(f\"\\r[{bar}] 0/{len(sentences)}\")\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        if include_dot and not sentence.endswith('.'): sentence += '.'\n",
    "        if include_paraphrase: sentence = \"paraphrase: \" + sentence\n",
    "        out = pipeline(sentence, **pipeline_args)\n",
    "        corrected_sentences.append(out[0]['generated_text'].strip())\n",
    "        progress = (idx + 1) / len(sentences)\n",
    "        filled_len = int(bar_len * progress)\n",
    "        bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "        print(f\"\\r[{bar}] {idx + 1}/{len(sentences)}\")\n",
    "\n",
    "    print(\"\\nCorrected Text:\")\n",
    "    final_text = ' '.join(corrected_sentences)\n",
    "    print(final_text + \"\\n\")\n",
    "    return final_text\n",
    "\n",
    "\n",
    "pipeline1 = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"humarin/chatgpt_paraphraser_on_T5_base\",\n",
    "    tokenizer=\"humarin/chatgpt_paraphraser_on_T5_base\"\n",
    ")\n",
    "\n",
    "pipeline2 = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"stanford-oval/paraphraser-bart-large\",\n",
    "    tokenizer=\"stanford-oval/paraphraser-bart-large\"\n",
    ")\n",
    "\n",
    "pipeline3 = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"tuner007/pegasus_paraphrase\",\n",
    "    tokenizer=\"tuner007/pegasus_paraphrase\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "input_text1 = (\"Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in \"\n",
    "              \"our lives. Hope you too, to enjoy it as my deepest wishes. Thank your message to show our words to the \"\n",
    "              \"doctor, as his next contract checking, to all of us. I got this message to see the approved message. \"\n",
    "              \"In fact, I have received the message from the professor, to show me, this, a couple of days ago. \"\n",
    "              \"I am very appreciated the full support of the professor, for our Springer proceedings publication\")\n",
    "\n",
    "input_text2 = (\"During our final discuss, I told him about the new submission — the one we were waiting since last \"\n",
    "              \"autumn, but the updates was confusing as it not included the full feedback from reviewer or maybe \"\n",
    "              \"editor? Anyway, I believe the team, although bit delay and less communication at recent days, they \"\n",
    "              \"really tried best for paper and cooperation. We should be grateful, I mean all of us, for the \"\n",
    "              \"acceptance and efforts until the Springer link came finally last week, I think. Also, kindly remind me \"\n",
    "              \"please, if the doctor still plan for the acknowledgments section edit before he sending again. \"\n",
    "              \"Because I didn’t see that part final yet, or maybe I missed, I apologize if so. Overall, \"\n",
    "              \"let us make sure all are safe and celebrate the outcome with strong coffee and future targets\")\n",
    "\n",
    "\n",
    "outputs = []\n",
    "print(\"Using Model 1: humarin/chatgpt_paraphraser_on_T5_base\\n\")\n",
    "outputs.append(paraphrase_text(True, False, False, pipeline1, input_text1))\n",
    "outputs.append(paraphrase_text(True,  False, False, pipeline1, input_text2, do_sample = True, temperature=0.8))\n",
    "\n",
    "print(\"Using Model 2: stanford-oval/paraphraser-bart-large\\n\")\n",
    "outputs.append(paraphrase_text(False, True, True, pipeline2, input_text1))\n",
    "outputs.append(paraphrase_text(False, True, True, pipeline2, input_text2))\n",
    "print(\"Using Model 3: tuner007/pegasus_paraphrase\\n\")\n",
    "outputs.append(paraphrase_text(True, False, False, pipeline3, input_text1,do_sample = True, temperature=1.1 ,num_beams=10, repetition_penalty=2.0, top_k=30, top_p=0.8))\n",
    "outputs.append(paraphrase_text(True, False, True, pipeline3, input_text2, do_sample = True, temperature=1.1 ,num_beams=10, repetition_penalty=2.0, top_k=30, top_p=0.8))\n",
    "\n",
    "with open(\"outputs_of_B.txt\", \"w\") as f:\n",
    "    for i, output in enumerate(outputs):\n",
    "        f.write(f\"Output {i+1}:\\n{output}\\n\\n\")\n",
    "\n",
    "print(\"All outputs have been written to outputs_of_B.txt\")"
   ],
   "id": "3a83e44f51eb9fca",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at tuner007/pegasus_paraphrase and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Model 1: humarin/chatgpt_paraphraser_on_T5_base\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Original Text:\n",
      "Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in our lives. Hope you too, to enjoy it as my deepest wishes. Thank your message to show our words to the doctor, as his next contract checking, to all of us. I got this message to see the approved message. In fact, I have received the message from the professor, to show me, this, a couple of days ago. I am very appreciated the full support of the professor, for our Springer proceedings publication\n",
      "\n",
      "Processing sentences: \n",
      "\n",
      "[------------------------------] 0/6\n",
      "[=====-------------------------] 1/6\n",
      "[==========--------------------] 2/6\n",
      "[===============---------------] 3/6\n",
      "[====================----------] 4/6\n",
      "[=========================-----] 5/6\n",
      "[==============================] 6/6\n",
      "\n",
      "Corrected Text:\n",
      "Our Chinese culture celebrates the dragon boat festival today to ensure the safety and prosperity of all involved. May you also have a good time, as my thoughts are with you. We appreciate your message to convey our thanks to the doctor for sharing our thoughts as they prepare for their next contract check. I was notified to view the sanctioned message. The professor sent me a message to show me this a few days ago. The professor's complete backing has been greatly appreciated for publishing our Springer proceedings.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Original Text:\n",
      "During our final discuss, I told him about the new submission — the one we were waiting since last autumn, but the updates was confusing as it not included the full feedback from reviewer or maybe editor? Anyway, I believe the team, although bit delay and less communication at recent days, they really tried best for paper and cooperation. We should be grateful, I mean all of us, for the acceptance and efforts until the Springer link came finally last week, I think. Also, kindly remind me please, if the doctor still plan for the acknowledgments section edit before he sending again. Because I didn’t see that part final yet, or maybe I missed, I apologize if so. Overall, let us make sure all are safe and celebrate the outcome with strong coffee and future targets\n",
      "\n",
      "Processing sentences: \n",
      "\n",
      "[------------------------------] 0/5\n",
      "[======------------------------] 1/5\n",
      "[============------------------] 2/5\n",
      "[==================------------] 3/5\n",
      "[========================------] 4/5\n",
      "[==============================] 5/5\n",
      "\n",
      "Corrected Text:\n",
      "During our last discussion, I shared with him the news of the new submission we had been waiting for last autumn, but the changes were unclear as they did not include full feedback from the reviewer or editor. Despite the delay and reduced communication, the team made efforts to produce papers and collaborate effectively. The acceptance and efforts made until the Springer link finally arrived last week should be acknowledged by everyone. Kindly inform me if the doctor has time to revise the acknowledgments section before resending. Is it possible that I didn't witness the final segment or was I missing something? I'm sorry if this appears. In the end, we must ensure that everyone is secure and mark the occasion with potent coffee and future objectives.\n",
      "\n",
      "Using Model 2: stanford-oval/paraphraser-bart-large\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Original Text:\n",
      "Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in our lives. Hope you too, to enjoy it as my deepest wishes. Thank your message to show our words to the doctor, as his next contract checking, to all of us. I got this message to see the approved message. In fact, I have received the message from the professor, to show me, this, a couple of days ago. I am very appreciated the full support of the professor, for our Springer proceedings publication\n",
      "\n",
      "Processing sentences: \n",
      "\n",
      "[------------------------------] 0/6\n",
      "[=====-------------------------] 1/6\n",
      "[==========--------------------] 2/6\n",
      "[===============---------------] 3/6\n",
      "[====================----------] 4/6\n",
      "[=========================-----] 5/6\n",
      "[==============================] 6/6\n",
      "\n",
      "Corrected Text:\n",
      "Today is our Dragon Boat Festival. In Chinese culture, we celebrate this with everything safe and beautiful in our lives. I hope you enjoy this as much as I do, as is my deepest wish. Thank you for your report showing our words to the doctor, as well as his next contractual check-up, for all of us. I received this message so I could see the approved report. Actually, I got a text from the professor to show me this a few days ago. I really appreciate the professor's full support in our Springer publication.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Original Text:\n",
      "During our final discuss, I told him about the new submission — the one we were waiting since last autumn, but the updates was confusing as it not included the full feedback from reviewer or maybe editor? Anyway, I believe the team, although bit delay and less communication at recent days, they really tried best for paper and cooperation. We should be grateful, I mean all of us, for the acceptance and efforts until the Springer link came finally last week, I think. Also, kindly remind me please, if the doctor still plan for the acknowledgments section edit before he sending again. Because I didn’t see that part final yet, or maybe I missed, I apologize if so. Overall, let us make sure all are safe and celebrate the outcome with strong coffee and future targets\n",
      "\n",
      "Processing sentences: \n",
      "\n",
      "[------------------------------] 0/6\n",
      "[=====-------------------------] 1/6\n",
      "[==========--------------------] 2/6\n",
      "[===============---------------] 3/6\n",
      "[====================----------] 4/6\n",
      "[=========================-----] 5/6\n",
      "[==============================] 6/6\n",
      "\n",
      "Corrected Text:\n",
      "During our last discussion, I told him about a new submission -- the one we had been waiting for since last fall, but the update was confusing because it didn't include full feedback from a reviewer or even an editor. Anyway, I trust that the team, even if a little late and less communicative in the last few days, has really tried its best in terms of paper and cooperation. I think we should all be grateful, I mean all of us, for accepting and trying until the Springer connection finally came through last week. Also, please remind me if the doctor is still planning to edit the acknowledgements section before sending it again. Since I haven't seen the final part yet, or maybe I missed it, I'm sorry if I did. All in all, let's make sure everyone is safe and celebrate the results with strong coffee and future goals.\n",
      "\n",
      "Using Model 3: tuner007/pegasus_paraphrase\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Original Text:\n",
      "Today is our dragon boat festival, in our Chinese culture, to celebrate it with all safe and great in our lives. Hope you too, to enjoy it as my deepest wishes. Thank your message to show our words to the doctor, as his next contract checking, to all of us. I got this message to see the approved message. In fact, I have received the message from the professor, to show me, this, a couple of days ago. I am very appreciated the full support of the professor, for our Springer proceedings publication\n",
      "\n",
      "Processing sentences: \n",
      "\n",
      "[------------------------------] 0/6\n",
      "[=====-------------------------] 1/6\n",
      "[==========--------------------] 2/6\n",
      "[===============---------------] 3/6\n",
      "[====================----------] 4/6\n",
      "[=========================-----] 5/6\n",
      "[==============================] 6/6\n",
      "\n",
      "Corrected Text:\n",
      "Today is the dragon boat festival in our Chinese culture and we should celebrate it with great joy. Hope you enjoy it as my deepest wishes. Thank you for your message, it will be shown to the doctor as his next contract checking. I received this message to see the approved one. I received the message from the professor a couple of days ago. The professor was very supportive of the Springer proceedings publication.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Original Text:\n",
      "During our final discuss, I told him about the new submission — the one we were waiting since last autumn, but the updates was confusing as it not included the full feedback from reviewer or maybe editor? Anyway, I believe the team, although bit delay and less communication at recent days, they really tried best for paper and cooperation. We should be grateful, I mean all of us, for the acceptance and efforts until the Springer link came finally last week, I think. Also, kindly remind me please, if the doctor still plan for the acknowledgments section edit before he sending again. Because I didn’t see that part final yet, or maybe I missed, I apologize if so. Overall, let us make sure all are safe and celebrate the outcome with strong coffee and future targets\n",
      "\n",
      "Processing sentences: \n",
      "\n",
      "[------------------------------] 0/6\n",
      "[=====-------------------------] 1/6\n",
      "[==========--------------------] 2/6\n",
      "[===============---------------] 3/6\n",
      "[====================----------] 4/6\n",
      "[=========================-----] 5/6\n",
      "[==============================] 6/6\n",
      "\n",
      "Corrected Text:\n",
      "During our final discuss, I told him about the new submission we were waiting for since last autumn, but it was confusing as it did not include the full feedback from reviewer or editor. I believe the team tried their best for paper and cooperation despite delay and less communication recently. We should be thankful for the acceptance and efforts until the Springer link came last week, I think. If the doctor still plans for the acknowledgments section to be edited before he sends again, please remind me. I apologize if I missed that part final because I didn't see it yet. Let us make sure all are safe and celebrate the outcome with coffee and targets.\n",
      "\n",
      "All outputs have been written to outputs_of_B.txt\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "C. Συγκρίνετε τα αποτελέσματα της κάθε προσέγγισης με τις κατάλληλες τεχνικές",
   "id": "cdd7c11b6e5ef712"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T21:41:17.985205Z",
     "start_time": "2025-09-22T21:41:17.201647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from nltk import PCFG\n",
    "from nltk.parse import ViterbiParser\n",
    "\n",
    "# Define grammar\n",
    "grammar = PCFG.fromstring(r\"\"\"\n",
    "# Top-level S\n",
    "S -> NP VP [0.60] | VP [0.20] | Modal NP VP [0.10] | Modal VP [0.05] | NP [0.05]\n",
    "\n",
    "# Noun phrases\n",
    "NP -> Pron [0.18] | Det N1 [0.18] | N [0.12] | Det Adj N [0.18] | Det N [0.18] | NP PP [0.16]\n",
    "N1 -> Adj N [0.40] | N [0.30] | N INF [0.20] | N SBAR [0.10]\n",
    "\n",
    "# Verb phrases (probabilities sum to 1.0)\n",
    "VP -> V NP [0.23]\n",
    "VP -> V [0.05]\n",
    "VP -> V NP PP [0.11]\n",
    "VP -> V NP INF [0.08]\n",
    "VP -> V S [0.03]\n",
    "VP -> Aux V NP [0.10]\n",
    "VP -> Aux V INF [0.04]\n",
    "VP -> Modal V NP [0.03]\n",
    "VP -> Modal V INF [0.02]\n",
    "VP -> Adv VP [0.03]\n",
    "VP -> V NP Adv [0.04]\n",
    "VP -> V NP SBAR [0.10]\n",
    "VP -> V NP SBAR SBAR [0.04]\n",
    "VP -> V NP Adv PP [0.06]\n",
    "VP -> Aux PP [0.04]\n",
    "\n",
    "# Infinitival complement\n",
    "INF -> 'to' VP [1.0]\n",
    "\n",
    "# Subordinate / comparative clauses\n",
    "SBAR -> Prep S [0.80] | Prep VP [0.15] | Prep Adv Prep S [0.05]\n",
    "\n",
    "# Prepositional phrase\n",
    "PP -> Prep NP [1.0]\n",
    "Prep -> 'as' [0.34] | 'so' [0.33] | 'with' [0.33]\n",
    "\n",
    "# Lexicon / preterminals\n",
    "Pron -> 'i' [0.125] | 'you' [0.125] | 'we' [0.125] | 'it' [0.125] | 'my' [0.125] | 'me' [0.125] | 'this' [0.125] | 'one' [0.125]\n",
    "\n",
    "Det -> 'the' [0.20] | 'my' [0.20] | 'your' [0.20] | 'a' [0.20] | 'this' [0.20]\n",
    "\n",
    "Adj -> 'deepest' [0.20] | 'approved' [0.20] | 'good' [0.20] | 'sanctioned' [0.20] | 'much' [0.20]\n",
    "\n",
    "N -> 'review' [0.10] | 'content' [0.10] | 'wishes' [0.10] | 'wish' [0.10] | 'message' [0.10]\n",
    "N -> 'it' [0.10] | 'time' [0.10] | 'thoughts' [0.10] | 'report' [0.10] | 'one' [0.10]\n",
    "\n",
    "V -> 'hope' [0.083333] | 'enjoy' [0.083333] | 'got' [0.083333] | 'review' [0.083333]\n",
    "V -> 'approve' [0.083333] | 'have' [0.083333] | 'notified' [0.083333] | 'view' [0.083333]\n",
    "V -> 'received' [0.083333] | 'see' [0.083333] | 'do' [0.083333] | 'is' [0.083333]\n",
    "\n",
    "Modal -> 'may' [0.5] | 'could' [0.5]\n",
    "\n",
    "Aux -> 'was' [0.25] | 'have' [0.25] | 'do' [0.25] | 'are' [0.25]\n",
    "\n",
    "Adv -> 'too' [0.3333] | 'also' [0.3333] | 'much' [0.3333]\n",
    "\"\"\")\n",
    "\n",
    "parser = ViterbiParser(grammar)\n",
    "\n",
    "# Read sentences from file\n",
    "sentences = []\n",
    "with open('outputs_of_A.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line not in [\"Output 1:\", \"Output 2:\", \"\"]:\n",
    "            sentences.append(line)\n",
    "\n",
    "with open('outputs_of_B.txt', 'r', encoding='utf-8') as f:\n",
    "    flag = True\n",
    "    output = 0\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line not in [\"Output 1:\", \"Output 2:\", \"Output 3:\", \"Output 4:\", \"Output 5:\", \"Output 6:\", \"\"]:\n",
    "            if flag:\n",
    "                sentences_in_line = re.split(r'(?<=[.!?])\\s+', line)\n",
    "                if output == 1:\n",
    "                    sentences.append(sentences_in_line[2])\n",
    "                    sentences.append(sentences_in_line[4])\n",
    "                else:\n",
    "                    sentences.append(sentences_in_line[1])\n",
    "                    sentences.append(sentences_in_line[3])\n",
    "                flag = False\n",
    "                output += 1\n",
    "            else:\n",
    "                flag = True\n",
    "\n",
    "def clean_and_tokenize(sent):\n",
    "    # Remove punctuation and lowercase\n",
    "    clean = re.sub(r\"[^\\w\\s]\", \"\", sent).lower()\n",
    "    return clean.split()\n",
    "\n",
    "# Parse each sentence\n",
    "for sentence in sentences:\n",
    "    tokens = clean_and_tokenize(sentence)\n",
    "    print(f\"\\nParsing sentence: {' '.join(tokens)}\")\n",
    "\n",
    "    try:\n",
    "        grammar.check_coverage(tokens)\n",
    "    except ValueError as e:\n",
    "        print(\"Coverage error:\", e)\n",
    "        continue\n",
    "\n",
    "    parses = list(parser.parse(tokens))\n",
    "    if not parses:\n",
    "        print(\"No valid parse found.\")\n",
    "    else:\n",
    "        for tree in parses:\n",
    "            tree.pretty_print()\n",
    "            break\n"
   ],
   "id": "1a5a1200c7cc5b3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing sentence: i hope you enjoy it too with my deepest wishes\n",
      "      S                                                  \n",
      "  ____|__________                                         \n",
      " |               VP                                      \n",
      " |     __________|_________                               \n",
      " |    |                    S                             \n",
      " |    |     _______________|___                           \n",
      " |    |    |                   VP                        \n",
      " |    |    |      _____________|________                  \n",
      " |    |    |     |    |    |            PP               \n",
      " |    |    |     |    |    |    ________|_____            \n",
      " NP   |    NP    |    NP   |   |              NP         \n",
      " |    |    |     |    |    |   |     _________|______     \n",
      "Pron  V   Pron   V   Pron Adv Prep Det       Adj     N   \n",
      " |    |    |     |    |    |   |    |         |      |    \n",
      " i   hope you  enjoy  it  too with  my     deepest wishes\n",
      "\n",
      "\n",
      "Parsing sentence: i got your message to review the approved content\n",
      "          S                                                  \n",
      "  ________|________________                                   \n",
      " |                         VP                                \n",
      " |     ____________________|__________                        \n",
      " |    |        |                     INF                     \n",
      " |    |        |            __________|___                    \n",
      " |    |        |           |              VP                 \n",
      " |    |        |           |     _________|_____              \n",
      " NP   |        NP          |    |               NP           \n",
      " |    |    ____|_____      |    |      _________|________     \n",
      "Pron  V  Det         N     |    V    Det       Adj       N   \n",
      " |    |   |          |     |    |     |         |        |    \n",
      " i   got your     message  to review the     approved content\n",
      "\n",
      "\n",
      "Parsing sentence: may you also have a good time as my thoughts are with you\n",
      "       S                                                                  \n",
      "   ____|______________                                                     \n",
      "  |    |              VP                                                  \n",
      "  |    |     _________|_____________                                       \n",
      "  |    |    |                       VP                                    \n",
      "  |    |    |     __________________|______________                        \n",
      "  |    |    |    |        |                       SBAR                    \n",
      "  |    |    |    |        |          ______________|______                 \n",
      "  |    |    |    |        |         |                     S               \n",
      "  |    |    |    |        |         |         ____________|___             \n",
      "  |    |    |    |        |         |        |                VP          \n",
      "  |    |    |    |        |         |        |             ___|____        \n",
      "  |    |    |    |        |         |        |            |        PP     \n",
      "  |    |    |    |        |         |        |            |    ____|___    \n",
      "  |    NP   |    |        NP        |        NP           |   |        NP \n",
      "  |    |    |    |     ___|____     |     ___|_____       |   |        |   \n",
      "Modal Pron Adv   V   Det Adj   N   Prep Det        N     Aux Prep     Pron\n",
      "  |    |    |    |    |   |    |    |    |         |      |   |        |   \n",
      " may  you  also have  a  good time  as   my     thoughts are with     you \n",
      "\n",
      "\n",
      "Parsing sentence: i was notified to view the sanctioned message\n",
      "                   S                                 \n",
      "  _________________|___                               \n",
      " |                     VP                            \n",
      " |     ________________|____                          \n",
      " |    |     |              INF                       \n",
      " |    |     |       ________|___                      \n",
      " |    |     |      |            VP                   \n",
      " |    |     |      |    ________|______               \n",
      " NP   |     |      |   |               NP            \n",
      " |    |     |      |   |     __________|_________     \n",
      "Pron Aux    V      |   V   Det        Adj        N   \n",
      " |    |     |      |   |    |          |         |    \n",
      " i   was notified  to view the     sanctioned message\n",
      "\n",
      "\n",
      "Parsing sentence: i hope you enjoy this as much as i do as is my deepest wish\n",
      "      S                                                                             \n",
      "  ____|__________                                                                    \n",
      " |               VP                                                                 \n",
      " |     __________|_________                                                          \n",
      " |    |                    S                                                        \n",
      " |    |     _______________|______________                                           \n",
      " |    |    |                              VP                                        \n",
      " |    |    |      ________________________|_____________________                     \n",
      " |    |    |     |    |              |                         SBAR                 \n",
      " |    |    |     |    |              |                  ________|____                \n",
      " |    |    |     |    |             SBAR               |             S              \n",
      " |    |    |     |    |     _________|_________        |             |               \n",
      " |    |    |     |    |    |    |    |         S       |             VP             \n",
      " |    |    |     |    |    |    |    |     ____|___    |     ________|_____          \n",
      " NP   |    NP    |    NP   |    |    |    NP       VP  |    |              NP       \n",
      " |    |    |     |    |    |    |    |    |        |   |    |    __________|_____    \n",
      "Pron  V   Pron   V   Pron Prep Adv  Prep Pron      V  Prep  V  Det        Adj    N  \n",
      " |    |    |     |    |    |    |    |    |        |   |    |   |          |     |   \n",
      " i   hope you  enjoy this  as  much  as   i        do  as   is  my      deepest wish\n",
      "\n",
      "\n",
      "Parsing sentence: i received this message so i could see the approved report\n",
      "        S                                                             \n",
      "  ______|______                                                        \n",
      " |             VP                                                     \n",
      " |       ______|_____________________                                  \n",
      " |      |           |               SBAR                              \n",
      " |      |           |            ____|__________                       \n",
      " |      |           |           |               S                     \n",
      " |      |           |           |     __________|___                   \n",
      " |      |           |           |    |              VP                \n",
      " |      |           |           |    |      ________|_____             \n",
      " NP     |           NP          |    NP    |    |         NP          \n",
      " |      |       ____|_____      |    |     |    |    _____|_______     \n",
      "Pron    V     Det         N    Prep Pron Modal  V  Det   Adj      N   \n",
      " |      |      |          |     |    |     |    |   |     |       |    \n",
      " i   received this     message  so   i   could see the approved report\n",
      "\n",
      "\n",
      "Parsing sentence: hope you enjoy it as my deepest wishes\n",
      "      S                                         \n",
      "      |                                          \n",
      "      VP                                        \n",
      "  ____|_____                                     \n",
      " |          S                                   \n",
      " |     _____|_________                           \n",
      " |    |               VP                        \n",
      " |    |      _________|________                  \n",
      " |    |     |    |             PP               \n",
      " |    |     |    |     ________|_____            \n",
      " |    NP    |    NP   |              NP         \n",
      " |    |     |    |    |     _________|______     \n",
      " V   Pron   V   Pron Prep Det       Adj     N   \n",
      " |    |     |    |    |    |         |      |    \n",
      "hope you  enjoy  it   as   my     deepest wishes\n",
      "\n",
      "\n",
      "Parsing sentence: i received this message to see the approved one\n",
      "               S                                           \n",
      "  _____________|________________                            \n",
      " |                              VP                         \n",
      " |       _______________________|_______                    \n",
      " |      |           |                  INF                 \n",
      " |      |           |            _______|___                \n",
      " |      |           |           |           VP             \n",
      " |      |           |           |    _______|_____          \n",
      " NP     |           NP          |   |             NP       \n",
      " |      |       ____|_____      |   |    _________|______   \n",
      "Pron    V     Det         N     |   V  Det       Adj     N \n",
      " |      |      |          |     |   |   |         |      |  \n",
      " i   received this     message  to see the     approved one\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
